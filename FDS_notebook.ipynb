{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f81a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Random Forest] CV MSE: 1.5382, Test MSE: 1.3941\n",
      "[Extra Trees] CV MSE: 1.6033, Test MSE: 1.2386\n",
      "[Snap Boosting Machine] CV MSE: 1.7984, Test MSE: 1.7250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['snap_boosting_model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from snapml import SnapBoostingMachineRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"]\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------- #\n",
    "# Model 1: Random Forest #\n",
    "# ---------------------- #\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "rf_cv_mse = -np.mean(cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "\n",
    "print(f\"[Random Forest] CV MSE: {rf_cv_mse:.4f}, Test MSE: {rf_mse:.4f}\")\n",
    "joblib.dump(rf, \"random_forest_model.pkl\")\n",
    "\n",
    "# ----------------------- #\n",
    "# Model 2: Extra Trees    #\n",
    "# ----------------------- #\n",
    "et = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "et.fit(X_train, y_train)\n",
    "et_pred = et.predict(X_test)\n",
    "et_mse = mean_squared_error(y_test, et_pred)\n",
    "et_cv_mse = -np.mean(cross_val_score(et, X_train, y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "\n",
    "print(f\"[Extra Trees] CV MSE: {et_cv_mse:.4f}, Test MSE: {et_mse:.4f}\")\n",
    "joblib.dump(et, \"extra_trees_model.pkl\")\n",
    "\n",
    "# -------------------------------------- #\n",
    "# Model 3: Snap Boosting Machine (IBM)   #\n",
    "# -------------------------------------- #\n",
    "snap = SnapBoostingMachineRegressor(num_round=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "snap.fit(X_train, y_train.values)  # Convert y_train to a NumPy array\n",
    "snap_pred = snap.predict(X_test)\n",
    "snap_mse = mean_squared_error(y_test, snap_pred)\n",
    "\n",
    "# Manual cross-validation for SnapML\n",
    "snap_cv_scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_cv_train, y_cv_val = y_train.values[train_idx], y_train.values[val_idx]  # Convert to NumPy arrays\n",
    "\n",
    "    snap_cv = SnapBoostingMachineRegressor(\n",
    "        num_round=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    )\n",
    "    snap_cv.fit(X_cv_train, y_cv_train)\n",
    "    snap_cv_pred = snap_cv.predict(X_cv_val)\n",
    "    snap_cv_scores.append(mean_squared_error(y_cv_val, snap_cv_pred))\n",
    "\n",
    "snap_cv_mse = np.mean(snap_cv_scores)\n",
    "print(f\"[Snap Boosting Machine] CV MSE: {snap_cv_mse:.4f}, Test MSE: {snap_mse:.4f}\")\n",
    "joblib.dump(snap, \"snap_boosting_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36e16dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'eval_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[32m     34\u001b[39m eval_set = [(X_val, y_val)]\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mxg_clf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogloss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[32m     44\u001b[39m xg_pred = xg_clf.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: XGBClassifier.fit() got an unexpected keyword argument 'eval_metric'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"V\"]  # Assuming \"V\" is the target variable for classification\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train-validation-test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xg_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # Use 'multi:softprob' for multi-class classification\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "eval_set = [(X_val, y_val)]\n",
    "xg_clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "xg_pred = xg_clf.predict(X_test)\n",
    "xg_accuracy = accuracy_score(y_test, xg_pred)\n",
    "xg_logloss = log_loss(y_test, xg_clf.predict_proba(X_test))\n",
    "\n",
    "# Cross-validation\n",
    "xg_cv_scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_cv_train, y_cv_val = y_train.values[train_idx], y_train.values[val_idx]\n",
    "\n",
    "    xg_cv = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        n_estimators=xg_clf.best_ntree_limit,  # Use the best number of trees found\n",
    "        random_state=42\n",
    "    )\n",
    "    xg_cv.fit(X_cv_train, y_cv_train)\n",
    "    xg_cv_pred = xg_cv.predict_proba(X_cv_val)\n",
    "    xg_cv_scores.append(log_loss(y_cv_val, xg_cv_pred))\n",
    "\n",
    "xg_cv_logloss = np.mean(xg_cv_scores)\n",
    "print(f\"[XGBoost Classifier with Early Stopping] CV Log Loss: {xg_cv_logloss:.4f}, Test Accuracy: {xg_accuracy:.4f}, Test Log Loss: {xg_logloss:.4f}\")\n",
    "joblib.dump(xg_clf, \"xgboost_classifier_early_stopping_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff652b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 8 with best iteration 3\n",
      "[Extra Trees with Early Stopping] CV MSE: 1.9768, Test MSE: 1.8322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['extra_trees_early_stopping_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"]\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train-validation-test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Early stopping parameters\n",
    "n_estimators = 100\n",
    "tolerance = 5  # Number of iterations to wait before stopping\n",
    "min_delta = 0.001  # Minimum change to qualify as an improvement\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_score = np.inf\n",
    "best_iteration = 0\n",
    "patience = 0\n",
    "\n",
    "# Train the ExtraTreesRegressor with early stopping\n",
    "et = ExtraTreesRegressor(warm_start=True, random_state=42)\n",
    "\n",
    "for i in range(1, n_estimators + 1):\n",
    "    et.set_params(n_estimators=i)\n",
    "    et.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    val_pred = et.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, val_pred)\n",
    "\n",
    "    # Check for improvement\n",
    "    if best_score - val_mse > min_delta:\n",
    "        best_score = val_mse\n",
    "        best_iteration = i\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience >= tolerance:\n",
    "        print(f\"Early stopping at iteration {i} with best iteration {best_iteration}\")\n",
    "        break\n",
    "\n",
    "# Train the final model with the best number of estimators\n",
    "et_final = ExtraTreesRegressor(n_estimators=best_iteration, random_state=42)\n",
    "et_final.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "et_pred = et_final.predict(X_test)\n",
    "et_mse = mean_squared_error(y_test, et_pred)\n",
    "\n",
    "# Cross-validation\n",
    "et_cv_scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_cv_train, y_cv_val = y_train.values[train_idx], y_train.values[val_idx]\n",
    "\n",
    "    et_cv = ExtraTreesRegressor(n_estimators=best_iteration, random_state=42)\n",
    "    et_cv.fit(X_cv_train, y_cv_train)\n",
    "    et_cv_pred = et_cv.predict(X_cv_val)\n",
    "    et_cv_scores.append(mean_squared_error(y_cv_val, et_cv_pred))\n",
    "\n",
    "et_cv_mse = np.mean(et_cv_scores)\n",
    "print(f\"[Extra Trees with Early Stopping] CV MSE: {et_cv_mse:.4f}, Test MSE: {et_mse:.4f}\")\n",
    "joblib.dump(et_final, \"extra_trees_early_stopping_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27d997c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-08 06:24:50,987] A new study created in memory with name: no-name-6cec9a82-7c3d-44e5-bc09-54512edf3974\n",
      "/tmp/ipykernel_3693/633357719.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 0.01, 0.3),\n",
      "/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [06:24:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "[W 2025-04-08 06:24:50,992] Trial 0 failed with parameters: {'xgb_learning_rate': 0.14451110017480837, 'xgb_max_depth': 9, 'xgb_n_estimators': 457} because of the following error: XGBoostError('[06:24:50] /workspace/src/objective/./regression_loss.h:69: Check failed: base_score > 0.0f && base_score < 1.0f: base_score must be in (0,1) for logistic loss, got: 1\\nStack trace:\\n  [bt] (0) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6acc) [0x769e700a6acc]\\n  [bt] (1) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xed8329) [0x769e70cd8329]\\n  [bt] (2) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x681223) [0x769e70481223]\\n  [bt] (3) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x6815ec) [0x769e704815ec]\\n  [bt] (4) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x68bb3b) [0x769e7048bb3b]\\n  [bt] (5) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x77) [0x769e6ffb6ba7]\\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x769ec02e0ff5]\\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x769ec02e040a]\\n  [bt] (8) /home/codespace/.python/current/lib/python3.12/lib-dynload/_ctypes.cpython-312-x86_64-linux-gnu.so(+0x13a46) [0x769ec0303a46]\\n\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3693/633357719.py\", line 46, in objective\n",
      "    xg_clf = xgb.train(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:24:50] /workspace/src/objective/./regression_loss.h:69: Check failed: base_score > 0.0f && base_score < 1.0f: base_score must be in (0,1) for logistic loss, got: 1\n",
      "Stack trace:\n",
      "  [bt] (0) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6acc) [0x769e700a6acc]\n",
      "  [bt] (1) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xed8329) [0x769e70cd8329]\n",
      "  [bt] (2) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x681223) [0x769e70481223]\n",
      "  [bt] (3) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x6815ec) [0x769e704815ec]\n",
      "  [bt] (4) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x68bb3b) [0x769e7048bb3b]\n",
      "  [bt] (5) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x77) [0x769e6ffb6ba7]\n",
      "  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x769ec02e0ff5]\n",
      "  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x769ec02e040a]\n",
      "  [bt] (8) /home/codespace/.python/current/lib/python3.12/lib-dynload/_ctypes.cpython-312-x86_64-linux-gnu.so(+0x13a46) [0x769ec0303a46]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-08 06:24:50,997] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[06:24:50] /workspace/src/objective/./regression_loss.h:69: Check failed: base_score > 0.0f && base_score < 1.0f: base_score must be in (0,1) for logistic loss, got: 1\nStack trace:\n  [bt] (0) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6acc) [0x769e700a6acc]\n  [bt] (1) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xed8329) [0x769e70cd8329]\n  [bt] (2) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x681223) [0x769e70481223]\n  [bt] (3) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x6815ec) [0x769e704815ec]\n  [bt] (4) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x68bb3b) [0x769e7048bb3b]\n  [bt] (5) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x77) [0x769e6ffb6ba7]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x769ec02e0ff5]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x769ec02e040a]\n  [bt] (8) /home/codespace/.python/current/lib/python3.12/lib-dynload/_ctypes.cpython-312-x86_64-linux-gnu.so(+0x13a46) [0x769ec0303a46]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Create a study and optimize\u001b[39;00m\n\u001b[32m     89\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Output the best trial\u001b[39;00m\n\u001b[32m     93\u001b[39m trial = study.best_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     43\u001b[39m evals = [(dtrain, \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m), (dval, \u001b[33m'\u001b[39m\u001b[33meval\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m     44\u001b[39m evals_result = {}\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m xg_clf = \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxgb_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_estimators\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m xg_pred = xg_clf.predict(xgb.DMatrix(X_test))\n\u001b[32m     57\u001b[39m xg_pred_binary = [\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred > \u001b[32m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m xg_pred]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/core.py:2246\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2246\u001b[39m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/core.py:310\u001b[39m, in \u001b[36m_check_call\u001b[39m\u001b[34m(ret)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m    return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[31mXGBoostError\u001b[39m: [06:24:50] /workspace/src/objective/./regression_loss.h:69: Check failed: base_score > 0.0f && base_score < 1.0f: base_score must be in (0,1) for logistic loss, got: 1\nStack trace:\n  [bt] (0) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6acc) [0x769e700a6acc]\n  [bt] (1) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xed8329) [0x769e70cd8329]\n  [bt] (2) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x681223) [0x769e70481223]\n  [bt] (3) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x6815ec) [0x769e704815ec]\n  [bt] (4) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x68bb3b) [0x769e7048bb3b]\n  [bt] (5) /workspaces/FDS_MiniProject/fds/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x77) [0x769e6ffb6ba7]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x769ec02e0ff5]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x769ec02e040a]\n  [bt] (8) /home/codespace/.python/current/lib/python3.12/lib-dynload/_ctypes.cpython-312-x86_64-linux-gnu.so(+0x13a46) [0x769ec0303a46]\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y_classification = df[\"V\"].apply(lambda x: 1 if x > 0 else 0)  # Ensure binary classification\n",
    "y_regression = df[\"M\"]  # Target for regression\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train-validation-test sets\n",
    "X_train, X_temp, y_train_class, y_temp_class = train_test_split(X_scaled, y_classification, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val_class, y_test_class = train_test_split(X_temp, y_temp_class, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train, X_temp, y_train_reg, y_temp_reg = train_test_split(X_scaled, y_regression, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val_reg, y_test_reg = train_test_split(X_temp, y_temp_reg, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters for XGBoost Classifier\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('xgb_max_depth', 3, 10),\n",
    "        'n_estimators': trial.suggest_int('xgb_n_estimators', 50, 500),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train_class)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val_class)\n",
    "    evals = [(dtrain, 'train'), (dval, 'eval')]\n",
    "    evals_result = {}\n",
    "\n",
    "    xg_clf = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=xgb_params['n_estimators'],\n",
    "        evals=evals,\n",
    "        evals_result=evals_result,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    xg_pred = xg_clf.predict(xgb.DMatrix(X_test))\n",
    "    xg_pred_binary = [1 if pred > 0.5 else 0 for pred in xg_pred]\n",
    "    xg_accuracy = accuracy_score(y_test_class, xg_pred_binary)\n",
    "\n",
    "    # Suggest hyperparameters for Extra Trees Classifier\n",
    "    et_params = {\n",
    "        'n_estimators': trial.suggest_int('et_n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('et_max_depth', 3, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    et_clf = ExtraTreesClassifier(**et_params)\n",
    "    et_clf.fit(X_train, y_train_class)\n",
    "    et_pred = et_clf.predict(X_test)\n",
    "    et_accuracy = accuracy_score(y_test_class, et_pred)\n",
    "\n",
    "    # Suggest hyperparameters for Random Forest Regressor\n",
    "    rf_params = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 3, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    rf_reg = RandomForestRegressor(**rf_params)\n",
    "    rf_reg.fit(X_train, y_train_reg)\n",
    "    rf_pred = rf_reg.predict(X_test)\n",
    "    rf_mse = mean_squared_error(y_test_reg, rf_pred)\n",
    "\n",
    "    # Combine metrics for optimization\n",
    "    combined_metric = xg_accuracy + et_accuracy - rf_mse\n",
    "    return combined_metric\n",
    "\n",
    "# Create a study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best trial\n",
    "trial = study.best_trial\n",
    "print(\"Best Trial:\")\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Save the best models\n",
    "best_xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': trial.params['xgb_learning_rate'],\n",
    "    'max_depth': trial.params['xgb_max_depth'],\n",
    "    'n_estimators': trial.params['xgb_n_estimators'],\n",
    "    'random_state': 42\n",
    "}\n",
    "best_xg_clf = xgb.train(\n",
    "    best_xgb_params,\n",
    "    xgb.DMatrix(X_train, label=y_train_class),\n",
    "    num_boost_round=best_xgb_params['n_estimators']\n",
    ")\n",
    "joblib.dump(best_xg_clf, \"best_xgboost_classifier.pkl\")\n",
    "\n",
    "best_et_params = {\n",
    "    'n_estimators': trial.params['et_n_estimators'],\n",
    "    'max_depth': trial.params['et_max_depth'],\n",
    "    'random_state': 42\n",
    "}\n",
    "best_et_clf = ExtraTreesClassifier(**best_et_params)\n",
    "best_et_clf.fit(X_train, y_train_class)\n",
    "joblib.dump(best_et_clf, \"best_extra_trees_classifier.pkl\")\n",
    "\n",
    "best_rf_params = {\n",
    "    'n_estimators': trial.params['rf_n_estimators'],\n",
    "    'max_depth': trial.params['rf_max_depth'],\n",
    "    'random_state': 42\n",
    "}\n",
    "best_rf_reg = RandomForestRegressor(**best_rf_params)\n",
    "best_rf_reg.fit(X_train, y_train_reg)\n",
    "joblib.dump(best_rf_reg, \"best_random_forest_regressor.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
