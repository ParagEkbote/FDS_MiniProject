{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost Accuracy: 0.4706\n",
      "ðŸ•’ Training Time: 51.92 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.64      0.56        11\n",
      "           1       0.92      0.92      0.92        12\n",
      "           2       0.47      0.44      0.45        16\n",
      "           3       0.25      0.36      0.30        11\n",
      "           4       0.27      0.17      0.21        18\n",
      "\n",
      "    accuracy                           0.47        68\n",
      "   macro avg       0.48      0.50      0.49        68\n",
      "weighted avg       0.47      0.47      0.46        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_model.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\",\"V\"])\n",
    "y = df[\"M\"]-1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = XGBClassifier(\n",
    "    eval_metric=\"logloss\",  \n",
    "    booster=\"dart\",  # Dropout-based boosting  \n",
    "    n_estimators=1000,  \n",
    "    max_depth=10,  \n",
    "    learning_rate=0.7,  \n",
    "    subsample=0.8,  \n",
    "    colsample_bytree=0.8  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… XGBoost Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"xgboost_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Mean Squared Error: 2.1186\n",
      "Test Mean Squared Error: 1.7505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"]\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform features into polynomial features\n",
    "poly = PolynomialFeatures(degree=3)  # Experimenting with a higher degree\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Train polynomial regression model with regularization (Ridge regression)\n",
    "ridge = Ridge(alpha=1.0)  # Regularization strength can be tuned\n",
    "ridge.fit(X_train_poly, y_train)\n",
    "\n",
    "# Cross-validation for better generalization\n",
    "cv_scores = cross_val_score(ridge, X_train_poly, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_mse = -np.mean(cv_scores)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = ridge.predict(X_test_poly)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Cross-Validation Mean Squared Error: {cv_mse:.4f}\")\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(ridge, \"ridge_polynomial_regression_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-25 09:50:36,271] A new study created in memory with name: no-name-c65edbb6-daec-4041-9b56-aea56167abb2\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,284] Trial 0 finished with value: 2.1022404827753 and parameters: {'alpha': 3.386731936774999, 'degree': 3}. Best is trial 0 with value: 2.1022404827753.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,307] Trial 1 finished with value: 2.1748679768540606 and parameters: {'alpha': 4.194861223459453, 'degree': 5}. Best is trial 0 with value: 2.1022404827753.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,327] Trial 2 finished with value: 2.124896453814168 and parameters: {'alpha': 0.23368617932740898, 'degree': 3}. Best is trial 0 with value: 2.1022404827753.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,344] Trial 3 finished with value: 1.9240390879957978 and parameters: {'alpha': 0.2945422696207971, 'degree': 2}. Best is trial 3 with value: 1.9240390879957978.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,361] Trial 4 finished with value: 2.13814800816879 and parameters: {'alpha': 1.6565889736788988, 'degree': 4}. Best is trial 3 with value: 1.9240390879957978.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,377] Trial 5 finished with value: 2.1268617463278 and parameters: {'alpha': 0.011460076530523392, 'degree': 3}. Best is trial 3 with value: 1.9240390879957978.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,403] Trial 6 finished with value: 2.1268892025123547 and parameters: {'alpha': 0.008404156953910015, 'degree': 3}. Best is trial 3 with value: 1.9240390879957978.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,425] Trial 7 finished with value: 2.1692863641886575 and parameters: {'alpha': 0.0014916427591539665, 'degree': 4}. Best is trial 3 with value: 1.9240390879957978.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,444] Trial 8 finished with value: 1.9245575550740497 and parameters: {'alpha': 0.0011221350137817186, 'degree': 2}. Best is trial 3 with value: 1.9240390879957978.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,464] Trial 9 finished with value: 2.1693013378486574 and parameters: {'alpha': 0.0010828390527987458, 'degree': 4}. Best is trial 3 with value: 1.9240390879957978.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,499] Trial 10 finished with value: 1.9240370885299043 and parameters: {'alpha': 0.29568741775864427, 'degree': 2}. Best is trial 10 with value: 1.9240370885299043.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,520] Trial 11 finished with value: 1.923937534107066 and parameters: {'alpha': 0.3528392446057955, 'degree': 2}. Best is trial 11 with value: 1.923937534107066.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,542] Trial 12 finished with value: 1.9244418043206448 and parameters: {'alpha': 0.06602901880278576, 'degree': 2}. Best is trial 11 with value: 1.923937534107066.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,566] Trial 13 finished with value: 1.9230840457883331 and parameters: {'alpha': 0.853930956675229, 'degree': 2}. Best is trial 13 with value: 1.9230840457883331.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,588] Trial 14 finished with value: 1.9227562682298829 and parameters: {'alpha': 1.0518798937615963, 'degree': 2}. Best is trial 14 with value: 1.9227562682298829.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,612] Trial 15 finished with value: 2.2328759966181684 and parameters: {'alpha': 1.07464980261285, 'degree': 5}. Best is trial 14 with value: 1.9227562682298829.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,643] Trial 16 finished with value: 1.9148907119702923 and parameters: {'alpha': 7.0856493645382175, 'degree': 2}. Best is trial 16 with value: 1.9148907119702923.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,680] Trial 17 finished with value: 2.093511380376117 and parameters: {'alpha': 4.974381961923965, 'degree': 3}. Best is trial 16 with value: 1.9148907119702923.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,709] Trial 18 finished with value: 1.9129320916777135 and parameters: {'alpha': 9.177195417497156, 'degree': 2}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,733] Trial 19 finished with value: 2.0732318688771607 and parameters: {'alpha': 9.901353169279702, 'degree': 3}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,755] Trial 20 finished with value: 1.9244599093254056 and parameters: {'alpha': 0.05585418556802713, 'degree': 2}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,780] Trial 21 finished with value: 1.9133994227339723 and parameters: {'alpha': 8.645772637044493, 'degree': 2}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,803] Trial 22 finished with value: 1.9130608042447221 and parameters: {'alpha': 9.028583037189604, 'degree': 2}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,825] Trial 23 finished with value: 1.9206801100604303 and parameters: {'alpha': 2.3845445992374836, 'degree': 2}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,847] Trial 24 finished with value: 2.0780448050116025 and parameters: {'alpha': 8.536812503352296, 'degree': 3}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,902] Trial 25 finished with value: 1.9206007300464616 and parameters: {'alpha': 2.438433209366343, 'degree': 2}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,934] Trial 26 finished with value: 2.121215581720459 and parameters: {'alpha': 0.6690977018474713, 'degree': 3}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,955] Trial 27 finished with value: 1.9177007933421404 and parameters: {'alpha': 4.585037387333009, 'degree': 2}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,976] Trial 28 finished with value: 2.136587391872218 and parameters: {'alpha': 1.8151443822627673, 'degree': 4}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:36,997] Trial 29 finished with value: 2.100772666450534 and parameters: {'alpha': 3.6363515583988675, 'degree': 3}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,019] Trial 30 finished with value: 1.915591798468889 and parameters: {'alpha': 6.413876821893411, 'degree': 2}. Best is trial 18 with value: 1.9129320916777135.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,043] Trial 31 finished with value: 1.9123257936975837 and parameters: {'alpha': 9.901804286039509, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,064] Trial 32 finished with value: 1.9124379115568388 and parameters: {'alpha': 9.764629836061914, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,084] Trial 33 finished with value: 1.9183663145197716 and parameters: {'alpha': 4.0589775202755005, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,105] Trial 34 finished with value: 1.920018769979022 and parameters: {'alpha': 2.8407580715394776, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,125] Trial 35 finished with value: 2.1143142607656937 and parameters: {'alpha': 1.5588694825064595, 'degree': 3}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,147] Trial 36 finished with value: 1.9124247530495022 and parameters: {'alpha': 9.780651899350225, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,172] Trial 37 finished with value: 2.443200899667826 and parameters: {'alpha': 0.1281479853020697, 'degree': 5}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,200] Trial 38 finished with value: 2.1223180542267412 and parameters: {'alpha': 0.5359813214664001, 'degree': 3}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,227] Trial 39 finished with value: 1.9245256694328579 and parameters: {'alpha': 0.0189679330829764, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,253] Trial 40 finished with value: 2.093514214221986 and parameters: {'alpha': 4.973823741932406, 'degree': 3}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,282] Trial 41 finished with value: 1.9126695762148347 and parameters: {'alpha': 9.485837008342232, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,305] Trial 42 finished with value: 1.9172485409229736 and parameters: {'alpha': 4.955489233734729, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,342] Trial 43 finished with value: 1.919235461810495 and parameters: {'alpha': 3.40351879566046, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,362] Trial 44 finished with value: 1.9218054818655552 and parameters: {'alpha': 1.6445625195510085, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,383] Trial 45 finished with value: 1.9245540304953697 and parameters: {'alpha': 0.0030935098898088793, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,405] Trial 46 finished with value: 1.9157004957743957 and parameters: {'alpha': 6.312851143868202, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,425] Trial 47 finished with value: 2.098295526790891 and parameters: {'alpha': 9.720084342781249, 'degree': 4}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,447] Trial 48 finished with value: 1.9202934465463095 and parameters: {'alpha': 2.6492545804282464, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n",
      "/tmp/ipykernel_5455/1749345864.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
      "[I 2025-03-25 09:50:37,469] Trial 49 finished with value: 1.915424065289854 and parameters: {'alpha': 6.571375132681044, 'degree': 2}. Best is trial 31 with value: 1.9123257936975837.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'alpha': 9.901804286039509, 'degree': 2}\n",
      "Test Mean Squared Error: 1.7777\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"]\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-3, 10)  # Regularization strength\n",
    "    degree = trial.suggest_int('degree', 2, 5)  # Polynomial degree\n",
    "\n",
    "    # Transform features into polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    \n",
    "    # Ridge regression\n",
    "    model = Ridge(alpha=alpha)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_poly, y_train, cv=5, scoring=make_scorer(mean_squared_error, greater_is_better=False)\n",
    "    )\n",
    "    return -np.mean(cv_scores)  # Minimize the negative MSE\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "poly = PolynomialFeatures(degree=best_params['degree'])\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "final_model = Ridge(alpha=best_params['alpha'])\n",
    "final_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = final_model.predict(X_test_poly)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Mean Squared Error: {test_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.3462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random_forest_model.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"]\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    max_depth=10,      # Maximum depth of each tree\n",
    "    min_samples_split=2,  # Minimum samples to split an internal node\n",
    "    min_samples_leaf=1,   # Minimum samples at leaf nodes\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, \"random_forest_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"land_mines_dataset.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\"])\n",
    "y = df[\"M\"] - 1\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define Optuna objective function for LightGBM\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 10, 100)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", -1, 50)  # -1 means no limit\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1)\n",
    "    min_child_samples = trial.suggest_int(\"min_child_samples\", 5, 50)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "\n",
    "    # Create and evaluate LightGBM model\n",
    "    model = LGBMClassifier(\n",
    "        num_leaves=num_leaves,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        min_child_samples=min_child_samples,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        random_state=42\n",
    "    )\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    return scores.mean()  # Return mean cross-validation accuracy\n",
    "\n",
    "# Create study and optimize\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# Train model with best hyperparameters\n",
    "best_params = study.best_params\n",
    "model = LGBMClassifier(**best_params, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"âœ… Optimized LightGBM Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(model, \"optimized_lightgbm.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AdaBoost Accuracy: 0.5735\n",
      "ðŸ•’ Training Time: 0.95 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.91      0.77        11\n",
      "           1       0.92      0.92      0.92        12\n",
      "           2       0.47      0.44      0.45        16\n",
      "           3       0.35      0.55      0.43        11\n",
      "           4       0.56      0.28      0.37        18\n",
      "\n",
      "    accuracy                           0.57        68\n",
      "   macro avg       0.59      0.62      0.59        68\n",
      "weighted avg       0.58      0.57      0.56        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adaboost_model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize AdaBoost model with a base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=7)  # Base weak learner\n",
    "model = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.8\n",
    ")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… AdaBoost Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"adaboost_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost Accuracy: 0.5441\n",
      "ðŸ•’ Training Time: 0.40 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.82      0.75        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "           2       0.47      0.50      0.48        16\n",
      "           3       0.33      0.45      0.38        11\n",
      "           4       0.40      0.22      0.29        18\n",
      "\n",
      "    accuracy                           0.54        68\n",
      "   macro avg       0.55      0.58      0.56        68\n",
      "weighted avg       0.53      0.54      0.53        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_model.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Polynomial Feature Engineering (degree=2, only interactions)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Apply SMOTE for class balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_poly, y_train)\n",
    "\n",
    "# Initialize XGBoost model with DART boosting and better hyperparameters\n",
    "model = XGBClassifier(\n",
    "    eval_metric=\"logloss\",\n",
    "    booster=\"gbtree\",  # Dropout-based boosting\n",
    "    n_estimators=300,  # More trees\n",
    "    max_depth=7,  # Deeper trees\n",
    "    learning_rate=0.03,  # Lower LR for better learning\n",
    "    subsample=0.8,  # Use 80% of data per tree\n",
    "    colsample_bytree=0.8,  # Use 80% of features per tree\n",
    ")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test_poly)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… XGBoost Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"xgboost_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.238816\n",
      "0:\tlearn: 0.5592593\ttotal: 53.4ms\tremaining: 13.3s\n",
      "50:\tlearn: 0.8592593\ttotal: 111ms\tremaining: 432ms\n",
      "100:\tlearn: 0.9703704\ttotal: 157ms\tremaining: 231ms\n",
      "150:\tlearn: 0.9925926\ttotal: 207ms\tremaining: 135ms\n",
      "200:\tlearn: 1.0000000\ttotal: 254ms\tremaining: 62ms\n",
      "249:\tlearn: 1.0000000\ttotal: 298ms\tremaining: 0us\n",
      "âœ… CatBoost Accuracy: 0.5882\n",
      "ðŸ•’ Training Time: 0.38 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.91      0.80        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "           2       0.53      0.50      0.52        16\n",
      "           3       0.40      0.55      0.46        11\n",
      "           4       0.45      0.28      0.34        18\n",
      "\n",
      "    accuracy                           0.59        68\n",
      "   macro avg       0.59      0.63      0.60        68\n",
      "weighted avg       0.58      0.59      0.57        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['catboost_model.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = CatBoostClassifier(iterations=250, eval_metric=\"Accuracy\", verbose=50)\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… CatBoost Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"catboost_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Logistic Regression Accuracy: 0.4412\n",
      "ðŸ•’ Training Time: 0.01 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.87      0.65        15\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       0.25      0.15      0.19        13\n",
      "           3       0.00      0.00      0.00        13\n",
      "           4       0.08      0.08      0.08        13\n",
      "\n",
      "    accuracy                           0.44        68\n",
      "   macro avg       0.34      0.42      0.37        68\n",
      "weighted avg       0.36      0.44      0.39        68\n",
      "\n",
      "ðŸŽ‰ Model saved as 'logistic_regression_model.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V_log\"])\n",
    "y = df[\"M\"] - 1  # Adjust target variable if necessary\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, solver=\"lbfgs\", multi_class=\"auto\")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"logistic_regression_model.pkl\")\n",
    "print(\"ðŸŽ‰ Model saved as 'logistic_regression_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RandomForest Accuracy: 0.5294\n",
      "ðŸ•’ Training Time: 0.29 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.91      0.77        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "           2       0.42      0.31      0.36        16\n",
      "           3       0.38      0.55      0.44        11\n",
      "           4       0.33      0.22      0.27        18\n",
      "\n",
      "    accuracy                           0.53        68\n",
      "   macro avg       0.53      0.58      0.54        68\n",
      "weighted avg       0.50      0.53      0.51        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['randomforest_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\",\"V\"])\n",
    "y = df[\"M\"]-1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = RandomForestClassifier(n_estimators=250)\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… RandomForest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"randomforest_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90\n",
      "[LightGBM] [Info] Number of data points in the train set: 270, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score -1.504077\n",
      "[LightGBM] [Info] Start training from score -1.537979\n",
      "[LightGBM] [Info] Start training from score -1.686399\n",
      "[LightGBM] [Info] Start training from score -1.591089\n",
      "[LightGBM] [Info] Start training from score -1.748274\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "âœ… LightGBM Accuracy: 0.6176\n",
      "ðŸ•’ Training Time: 0.18 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "           2       0.59      0.62      0.61        16\n",
      "           3       0.38      0.45      0.42        11\n",
      "           4       0.54      0.39      0.45        18\n",
      "\n",
      "    accuracy                           0.62        68\n",
      "   macro avg       0.62      0.64      0.63        68\n",
      "weighted avg       0.61      0.62      0.61        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lightgbm_model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\",\"V\"])\n",
    "y = df[\"M\"]-1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = lgb.LGBMClassifier(n_estimators=300)\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… LightGBM Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"lightgbm_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ExtraTrees Accuracy: 0.4706\n",
      "ðŸ•’ Training Time: 0.24 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.91      0.74        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "           2       0.25      0.19      0.21        16\n",
      "           3       0.36      0.45      0.40        11\n",
      "           4       0.23      0.17      0.19        18\n",
      "\n",
      "    accuracy                           0.47        68\n",
      "   macro avg       0.46      0.53      0.49        68\n",
      "weighted avg       0.43      0.47      0.44        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['extratrees_model.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\",\"V\"])\n",
    "y = df[\"M\"]-1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = ExtraTreesClassifier(n_estimators=250, random_state=42)\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… ExtraTrees Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"extratrees_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SVM Accuracy: 0.3676\n",
      "ðŸ•’ Training Time: 0.02 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.82      0.37        11\n",
      "           1       1.00      0.92      0.96        12\n",
      "           2       0.20      0.12      0.15        16\n",
      "           3       0.50      0.09      0.15        11\n",
      "           4       0.29      0.11      0.16        18\n",
      "\n",
      "    accuracy                           0.37        68\n",
      "   macro avg       0.44      0.41      0.36        68\n",
      "weighted avg       0.42      0.37      0.33        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\",\"V\"])\n",
    "y = df[\"M\"]-1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = SVC(kernel=\"rbf\", probability=True)\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… SVM Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"svm_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\",\"V\"])\n",
    "y = df[\"M\"]-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': len(set(y)),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.03, 0.3),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 50, 200),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    return cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy').mean()\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Train final model\n",
    "best_params = study.best_params\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Final Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:54:09,492] A new study created in memory with name: no-name-11236871-8e43-4890-9aaa-21e3744e7481\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:09,674] Trial 0 finished with value: 0.9166666666666666 and parameters: {'learning_rate': 0.0464206217060879, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.717834288002983, 'colsample_bytree': 0.5348733935048986, 'lambda': 2.2306672981281233e-05, 'alpha': 0.00283373711338698}. Best is trial 0 with value: 0.9166666666666666.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:09,968] Trial 1 finished with value: 0.925 and parameters: {'learning_rate': 0.03483778754060309, 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.8956197927457255, 'colsample_bytree': 0.7891226008852466, 'lambda': 0.00017783006673477867, 'alpha': 5.273524231500938e-06}. Best is trial 1 with value: 0.925.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:09] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:10,196] Trial 2 finished with value: 0.925 and parameters: {'learning_rate': 0.1459478168376116, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.7657581484669163, 'colsample_bytree': 0.6066347120144668, 'lambda': 0.26935898986607554, 'alpha': 0.0006232469531192533}. Best is trial 1 with value: 0.925.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:10,334] Trial 3 finished with value: 0.925 and parameters: {'learning_rate': 0.022866142644844635, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.9066386884699005, 'colsample_bytree': 0.7143883006546471, 'lambda': 5.401885362572983e-07, 'alpha': 0.32136236809232177}. Best is trial 1 with value: 0.925.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:10,462] Trial 4 finished with value: 0.925 and parameters: {'learning_rate': 0.10877453981938044, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.9796190309728097, 'colsample_bytree': 0.7810065177098573, 'lambda': 4.703974315346142, 'alpha': 6.472295668079736e-06}. Best is trial 1 with value: 0.925.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:10,613] Trial 5 finished with value: 0.9333333333333332 and parameters: {'learning_rate': 0.025924035589837687, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.7332947439899592, 'colsample_bytree': 0.7418234672249371, 'lambda': 3.7830427816026845e-08, 'alpha': 1.8010364463156394}. Best is trial 5 with value: 0.9333333333333332.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:10,742] Trial 6 finished with value: 0.925 and parameters: {'learning_rate': 0.07091420558634338, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.9600575080045685, 'colsample_bytree': 0.9951666414228797, 'lambda': 0.07644364546264462, 'alpha': 5.486948288781925e-06}. Best is trial 5 with value: 0.9333333333333332.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:10,876] Trial 7 finished with value: 0.9333333333333333 and parameters: {'learning_rate': 0.12984019654807308, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.5689128425407883, 'colsample_bytree': 0.827013380059826, 'lambda': 0.15554985860487275, 'alpha': 0.0033423655292940703}. Best is trial 7 with value: 0.9333333333333333.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:11,040] Trial 8 finished with value: 0.9166666666666666 and parameters: {'learning_rate': 0.038607311977807986, 'max_depth': 5, 'min_child_weight': 9, 'subsample': 0.6337823705071011, 'colsample_bytree': 0.6324690119308821, 'lambda': 0.0001371641080867273, 'alpha': 0.0017024234086134328}. Best is trial 7 with value: 0.9333333333333333.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:11,289] Trial 9 finished with value: 0.9166666666666666 and parameters: {'learning_rate': 0.017139253759927274, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.9953140320074946, 'colsample_bytree': 0.7189386488164804, 'lambda': 3.0557940926450895e-07, 'alpha': 6.226902725002298e-08}. Best is trial 7 with value: 0.9333333333333333.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:11,679] Trial 10 finished with value: 0.9083333333333334 and parameters: {'learning_rate': 0.2514298549270335, 'max_depth': 12, 'min_child_weight': 1, 'subsample': 0.5017539201106913, 'colsample_bytree': 0.9299722609949245, 'lambda': 0.02415710567716449, 'alpha': 0.07969433145833466}. Best is trial 7 with value: 0.9333333333333333.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:11,884] Trial 11 finished with value: 0.9166666666666666 and parameters: {'learning_rate': 0.011399611537414395, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.5251785064132225, 'colsample_bytree': 0.866002294561673, 'lambda': 1.662411912106355e-08, 'alpha': 8.521260573292006}. Best is trial 7 with value: 0.9333333333333333.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:11] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:12,022] Trial 12 finished with value: 0.9166666666666666 and parameters: {'learning_rate': 0.29008117676681455, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6139960556554612, 'colsample_bytree': 0.8329125328097615, 'lambda': 0.006062149528122011, 'alpha': 7.264745450899863}. Best is trial 7 with value: 0.9333333333333333.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:12,192] Trial 13 finished with value: 0.9166666666666667 and parameters: {'learning_rate': 0.09889908324141733, 'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.7788133709760376, 'colsample_bytree': 0.6685222954202326, 'lambda': 2.174876204777018, 'alpha': 0.0707053166395468}. Best is trial 7 with value: 0.9333333333333333.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:12,404] Trial 14 finished with value: 0.925 and parameters: {'learning_rate': 0.1623403811191659, 'max_depth': 9, 'min_child_weight': 6, 'subsample': 0.6669070514727665, 'colsample_bytree': 0.8821438533281927, 'lambda': 0.0024387649727751737, 'alpha': 0.5769890453605018}. Best is trial 7 with value: 0.9333333333333333.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:12,618] Trial 15 finished with value: 0.9416666666666668 and parameters: {'learning_rate': 0.023874766876701002, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.8276324160849453, 'colsample_bytree': 0.766476796013486, 'lambda': 1.4160332371260775e-05, 'alpha': 0.011953030118598672}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:12,819] Trial 16 finished with value: 0.925 and parameters: {'learning_rate': 0.06161136081846622, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.8271197765349848, 'colsample_bytree': 0.9290338136286087, 'lambda': 6.836718223036415e-06, 'alpha': 0.015071781332331767}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:12,997] Trial 17 finished with value: 0.9333333333333333 and parameters: {'learning_rate': 0.01881973401167889, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.5720676547801655, 'colsample_bytree': 0.787156524934282, 'lambda': 5.700352229410507e-06, 'alpha': 6.427921070192784e-05}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:13,191] Trial 18 finished with value: 0.925 and parameters: {'learning_rate': 0.010654782146456434, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.8454801360531816, 'colsample_bytree': 0.846831733869043, 'lambda': 0.0012472942141577687, 'alpha': 0.00016813332328232536}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:13,376] Trial 19 finished with value: 0.9166666666666667 and parameters: {'learning_rate': 0.08459041217050835, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.683807296255428, 'colsample_bytree': 0.6726883232384399, 'lambda': 0.48035030619074454, 'alpha': 0.018344155227414994}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:13,526] Trial 20 finished with value: 0.925 and parameters: {'learning_rate': 0.17075574095215462, 'max_depth': 11, 'min_child_weight': 6, 'subsample': 0.8338751225428358, 'colsample_bytree': 0.5488797534620268, 'lambda': 3.9790886108132545e-05, 'alpha': 1.2564643679223275e-07}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:13,712] Trial 21 finished with value: 0.9333333333333333 and parameters: {'learning_rate': 0.01873686804380983, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.5479169471759066, 'colsample_bytree': 0.7905584083268836, 'lambda': 3.850612228868338e-06, 'alpha': 8.178320744017977e-05}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:13,907] Trial 22 finished with value: 0.925 and parameters: {'learning_rate': 0.016032705551702848, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.5739285402396782, 'colsample_bytree': 0.7871832652049283, 'lambda': 9.324965978400353e-07, 'alpha': 3.3935138497748e-05}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:14,091] Trial 23 finished with value: 0.925 and parameters: {'learning_rate': 0.02989878474377276, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.5954956573486138, 'colsample_bytree': 0.8248122940729576, 'lambda': 0.0006279315872123328, 'alpha': 0.003141558055129775}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:14,290] Trial 24 finished with value: 0.925 and parameters: {'learning_rate': 0.013763567872115311, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.659241511753732, 'colsample_bytree': 0.9052229876231779, 'lambda': 2.5826349974678233e-06, 'alpha': 3.659688830744915e-07}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:14,460] Trial 25 finished with value: 0.925 and parameters: {'learning_rate': 0.049187068300277054, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.5568348477996902, 'colsample_bytree': 0.7507243632713625, 'lambda': 2.3102575131836263e-05, 'alpha': 0.013701892995041787}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:14,662] Trial 26 finished with value: 0.925 and parameters: {'learning_rate': 0.024487627234104956, 'max_depth': 8, 'min_child_weight': 2, 'subsample': 0.7920474519715409, 'colsample_bytree': 0.9836214627493488, 'lambda': 1.383094631387022e-07, 'alpha': 0.0004992072637120695}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:14,823] Trial 27 finished with value: 0.9166666666666666 and parameters: {'learning_rate': 0.038867389534106504, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.702469998479232, 'colsample_bytree': 0.6924276512099838, 'lambda': 0.01036597488597572, 'alpha': 2.2848498139421736e-05}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:15,034] Trial 28 finished with value: 0.925 and parameters: {'learning_rate': 0.01999862413949997, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.8830839317388292, 'colsample_bytree': 0.817802835278393, 'lambda': 7.677573776201101e-05, 'alpha': 1.010785119805993e-06}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:15,227] Trial 29 finished with value: 0.9166666666666667 and parameters: {'learning_rate': 0.053034879641628216, 'max_depth': 9, 'min_child_weight': 2, 'subsample': 0.7230095440900892, 'colsample_bytree': 0.7500290951874582, 'lambda': 7.018569768621777e-06, 'alpha': 1.2597526607571402e-08}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:15,406] Trial 30 finished with value: 0.925 and parameters: {'learning_rate': 0.012830257994372365, 'max_depth': 10, 'min_child_weight': 7, 'subsample': 0.6426643742320922, 'colsample_bytree': 0.5041468328617431, 'lambda': 0.00031831287278391823, 'alpha': 0.0038382904667701922}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:15,587] Trial 31 finished with value: 0.9333333333333333 and parameters: {'learning_rate': 0.018816509108716472, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.544356875533026, 'colsample_bytree': 0.8062019317516695, 'lambda': 5.208410357863178e-06, 'alpha': 0.00010974211158355797}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:15,818] Trial 32 finished with value: 0.925 and parameters: {'learning_rate': 0.03020782263208038, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.585576761703428, 'colsample_bytree': 0.7704417248822273, 'lambda': 1.5519924086376323e-06, 'alpha': 2.4099296809911786e-05}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:16,013] Trial 33 finished with value: 0.925 and parameters: {'learning_rate': 0.015185248111463484, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.5204427610763291, 'colsample_bytree': 0.8694680705268263, 'lambda': 1.5812942645666467e-05, 'alpha': 0.0008378270828399019}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:16,176] Trial 34 finished with value: 0.9166666666666666 and parameters: {'learning_rate': 0.020243707898697547, 'max_depth': 4, 'min_child_weight': 6, 'subsample': 0.560651840078926, 'colsample_bytree': 0.8009145007253936, 'lambda': 3.11149912038644e-07, 'alpha': 0.00028643962209963264}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:16,337] Trial 35 finished with value: 0.9333333333333333 and parameters: {'learning_rate': 0.12998675482608396, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.6113543795366219, 'colsample_bytree': 0.7327909529170702, 'lambda': 5.708838081152517e-05, 'alpha': 0.00011795570245356442}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:16,500] Trial 36 finished with value: 0.9166666666666666 and parameters: {'learning_rate': 0.029759844633316244, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.5015092428353631, 'colsample_bytree': 0.7703928080910535, 'lambda': 5.983171925590197e-08, 'alpha': 0.006107384037890908}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:16,731] Trial 37 finished with value: 0.9333333333333333 and parameters: {'learning_rate': 0.04088191158837842, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.9289055841438483, 'colsample_bytree': 0.8443959864965779, 'lambda': 1.6231977388770105e-05, 'alpha': 1.5473690761921181e-06}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:16,994] Trial 38 finished with value: 0.925 and parameters: {'learning_rate': 0.023315971150048045, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.6210070748393727, 'colsample_bytree': 0.6376354690783581, 'lambda': 9.128016568060938, 'alpha': 0.05977162258572669}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:17,186] Trial 39 finished with value: 0.925 and parameters: {'learning_rate': 0.0643776194568819, 'max_depth': 6, 'min_child_weight': 8, 'subsample': 0.8816349233459098, 'colsample_bytree': 0.703684042173573, 'lambda': 0.0002461475413873939, 'alpha': 3.9033574990725155e-05}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:17,335] Trial 40 finished with value: 0.9333333333333333 and parameters: {'learning_rate': 0.18850454149439794, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.7575059740188441, 'colsample_bytree': 0.7660764204365786, 'lambda': 0.6357134028169755, 'alpha': 0.0013224256452873861}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:17,505] Trial 41 finished with value: 0.925 and parameters: {'learning_rate': 0.020206886520484756, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.5424167561403981, 'colsample_bytree': 0.8041413925725216, 'lambda': 4.653137365204151e-06, 'alpha': 0.00010739651842608261}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:17,680] Trial 42 finished with value: 0.9166666666666666 and parameters: {'learning_rate': 0.017796335473214835, 'max_depth': 6, 'min_child_weight': 6, 'subsample': 0.5391969503199839, 'colsample_bytree': 0.804732506289603, 'lambda': 2.591072217864515e-06, 'alpha': 1.0359310892839688e-05}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:17,914] Trial 43 finished with value: 0.925 and parameters: {'learning_rate': 0.027267705406593044, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.5920904933601829, 'colsample_bytree': 0.7254783599933468, 'lambda': 0.09432614883168405, 'alpha': 0.00031748602964297966}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:17] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:18,100] Trial 44 finished with value: 0.925 and parameters: {'learning_rate': 0.012372307983093108, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.5648559297054996, 'colsample_bytree': 0.8468473876559249, 'lambda': 1.1181538766178941e-06, 'alpha': 6.255400600526273e-05}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:18,271] Trial 45 finished with value: 0.925 and parameters: {'learning_rate': 0.03468451083980522, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.5213596453393183, 'colsample_bytree': 0.8824617532822407, 'lambda': 9.40083590560626e-06, 'alpha': 3.3250961843256065e-06}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:18,440] Trial 46 finished with value: 0.925 and parameters: {'learning_rate': 0.017648148920774417, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.807615172431819, 'colsample_bytree': 0.783183851428346, 'lambda': 0.00011467014622066727, 'alpha': 0.2637923215301313}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:18,622] Trial 47 finished with value: 0.925 and parameters: {'learning_rate': 0.010337402261419819, 'max_depth': 7, 'min_child_weight': 6, 'subsample': 0.5412548059404646, 'colsample_bytree': 0.8193138920494195, 'lambda': 5.381329178949935e-07, 'alpha': 1.2105502799932118e-05}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:18,833] Trial 48 finished with value: 0.925 and parameters: {'learning_rate': 0.01413963672027518, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.6363107352098514, 'colsample_bytree': 0.7500267258113228, 'lambda': 2.9206466257703736e-05, 'alpha': 0.0014422018534022642}. Best is trial 15 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_33632/382655930.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
      "/tmp/ipykernel_33632/382655930.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
      "/tmp/ipykernel_33632/382655930.py:23: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
      "/tmp/ipykernel_33632/382655930.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-03-23 14:54:19,058] Trial 49 finished with value: 0.925 and parameters: {'learning_rate': 0.021419601082105524, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.606307228035499, 'colsample_bytree': 0.9000297664266421, 'lambda': 1.3446686930672157e-07, 'alpha': 0.00834647930288339}. Best is trial 15 with value: 0.9416666666666668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 1.0000\n",
      "Best Parameters: {'learning_rate': 0.023874766876701002, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.8276324160849453, 'colsample_bytree': 0.766476796013486, 'lambda': 1.4160332371260775e-05, 'alpha': 0.011953030118598672}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:54:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': len(set(y)),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', **params)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    return cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy').mean()\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Train final model\n",
    "best_params = study.best_params\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', **best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Final Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 16:08:42,354] A new study created in memory with name: no-name-ec2f5ecb-3a96-4b91-86fd-629da7a7ce1b\n",
      "[I 2025-03-24 16:08:42,946] Trial 0 finished with value: 0.47407407407407404 and parameters: {'n_estimators': 262, 'max_depth': 12, 'learning_rate': 0.08191234991587981, 'min_child_weight': 9, 'subsample': 0.5609289280168472, 'colsample_bytree': 0.9957820201645056, 'lambda': 0.0236145022073437, 'alpha': 2.0673503150017725e-08, 'booster': 'gbtree'}. Best is trial 0 with value: 0.47407407407407404.\n",
      "[I 2025-03-24 16:08:46,072] Trial 1 finished with value: 0.4851851851851852 and parameters: {'n_estimators': 111, 'max_depth': 9, 'learning_rate': 0.04808861551365078, 'min_child_weight': 10, 'subsample': 0.8881956238107318, 'colsample_bytree': 0.8543781209700614, 'lambda': 0.05768598893849582, 'alpha': 2.3624256991243655e-07, 'booster': 'dart'}. Best is trial 1 with value: 0.4851851851851852.\n",
      "[I 2025-03-24 16:08:57,194] Trial 2 finished with value: 0.5074074074074074 and parameters: {'n_estimators': 218, 'max_depth': 11, 'learning_rate': 0.02271495556344563, 'min_child_weight': 9, 'subsample': 0.6191261628192205, 'colsample_bytree': 0.9320112475088915, 'lambda': 4.9087501361053285e-08, 'alpha': 1.770532089785307e-08, 'booster': 'dart'}. Best is trial 2 with value: 0.5074074074074074.\n",
      "[I 2025-03-24 16:08:57,459] Trial 3 finished with value: 0.46296296296296297 and parameters: {'n_estimators': 133, 'max_depth': 11, 'learning_rate': 0.2379729324371853, 'min_child_weight': 2, 'subsample': 0.6596598392175343, 'colsample_bytree': 0.6981677173347993, 'lambda': 0.009100985865749294, 'alpha': 9.007058449706376, 'booster': 'gbtree'}. Best is trial 2 with value: 0.5074074074074074.\n",
      "[I 2025-03-24 16:08:57,966] Trial 4 finished with value: 0.5222222222222223 and parameters: {'n_estimators': 141, 'max_depth': 5, 'learning_rate': 0.1433987809807424, 'min_child_weight': 3, 'subsample': 0.5960163969461204, 'colsample_bytree': 0.9166279060288378, 'lambda': 0.1516184832138677, 'alpha': 0.010730801380873851, 'booster': 'gbtree'}. Best is trial 4 with value: 0.5222222222222223.\n",
      "[I 2025-03-24 16:09:13,463] Trial 5 finished with value: 0.4703703703703703 and parameters: {'n_estimators': 254, 'max_depth': 4, 'learning_rate': 0.0751469522643144, 'min_child_weight': 1, 'subsample': 0.97095126584204, 'colsample_bytree': 0.7531233097720931, 'lambda': 2.8085658025187924e-06, 'alpha': 7.50833383491343e-05, 'booster': 'dart'}. Best is trial 4 with value: 0.5222222222222223.\n",
      "[I 2025-03-24 16:09:13,963] Trial 6 finished with value: 0.4888888888888888 and parameters: {'n_estimators': 162, 'max_depth': 3, 'learning_rate': 0.01277634839183789, 'min_child_weight': 6, 'subsample': 0.572759911819613, 'colsample_bytree': 0.7133603283331578, 'lambda': 0.12281369831957561, 'alpha': 3.245146380921008e-08, 'booster': 'gbtree'}. Best is trial 4 with value: 0.5222222222222223.\n",
      "[I 2025-03-24 16:09:14,500] Trial 7 finished with value: 0.4925925925925926 and parameters: {'n_estimators': 129, 'max_depth': 4, 'learning_rate': 0.028210757328942047, 'min_child_weight': 2, 'subsample': 0.8675199032940003, 'colsample_bytree': 0.8104300897755023, 'lambda': 0.26332131634544514, 'alpha': 0.012589504990883845, 'booster': 'gbtree'}. Best is trial 4 with value: 0.5222222222222223.\n",
      "[I 2025-03-24 16:09:33,021] Trial 8 finished with value: 0.47777777777777775 and parameters: {'n_estimators': 274, 'max_depth': 10, 'learning_rate': 0.08202507128492977, 'min_child_weight': 5, 'subsample': 0.9456164017486015, 'colsample_bytree': 0.5468202775552558, 'lambda': 1.3817190139268626e-08, 'alpha': 0.00011625303054767528, 'booster': 'dart'}. Best is trial 4 with value: 0.5222222222222223.\n",
      "[I 2025-03-24 16:09:41,659] Trial 9 finished with value: 0.4962962962962963 and parameters: {'n_estimators': 183, 'max_depth': 5, 'learning_rate': 0.011731353574129726, 'min_child_weight': 6, 'subsample': 0.7648172523969973, 'colsample_bytree': 0.8131825043601735, 'lambda': 6.64132769466474e-05, 'alpha': 1.102001602378926e-05, 'booster': 'dart'}. Best is trial 4 with value: 0.5222222222222223.\n",
      "[I 2025-03-24 16:09:42,221] Trial 10 finished with value: 0.47777777777777775 and parameters: {'n_estimators': 219, 'max_depth': 7, 'learning_rate': 0.2702275810876624, 'min_child_weight': 4, 'subsample': 0.5014256059042482, 'colsample_bytree': 0.599161688008399, 'lambda': 6.405145380039948, 'alpha': 0.09780431527806711, 'booster': 'gbtree'}. Best is trial 4 with value: 0.5222222222222223.\n",
      "[I 2025-03-24 16:09:53,683] Trial 11 finished with value: 0.5148148148148148 and parameters: {'n_estimators': 223, 'max_depth': 7, 'learning_rate': 0.14839517889348905, 'min_child_weight': 8, 'subsample': 0.6617902773052297, 'colsample_bytree': 0.9722526971365871, 'lambda': 1.2333510830983943e-08, 'alpha': 0.01038782132865939, 'booster': 'dart'}. Best is trial 4 with value: 0.5222222222222223.\n",
      "[I 2025-03-24 16:10:00,955] Trial 12 finished with value: 0.5074074074074074 and parameters: {'n_estimators': 172, 'max_depth': 7, 'learning_rate': 0.15200842687218014, 'min_child_weight': 7, 'subsample': 0.7117772194060428, 'colsample_bytree': 0.9980982136515972, 'lambda': 0.00014520608877827815, 'alpha': 0.010541822509756398, 'booster': 'dart'}. Best is trial 4 with value: 0.5222222222222223.\n",
      "[I 2025-03-24 16:10:01,724] Trial 13 finished with value: 0.5296296296296297 and parameters: {'n_estimators': 299, 'max_depth': 6, 'learning_rate': 0.15125161500144071, 'min_child_weight': 4, 'subsample': 0.776283013560285, 'colsample_bytree': 0.9076619528939874, 'lambda': 1.2018699239665022e-06, 'alpha': 1.0607180921688615, 'booster': 'gbtree'}. Best is trial 13 with value: 0.5296296296296297.\n",
      "[I 2025-03-24 16:10:02,064] Trial 14 finished with value: 0.5296296296296296 and parameters: {'n_estimators': 154, 'max_depth': 5, 'learning_rate': 0.13881529807950363, 'min_child_weight': 4, 'subsample': 0.7950491269341354, 'colsample_bytree': 0.8951265319485028, 'lambda': 1.8219802117574285e-06, 'alpha': 4.297055752573437, 'booster': 'gbtree'}. Best is trial 13 with value: 0.5296296296296297.\n",
      "[I 2025-03-24 16:10:02,572] Trial 15 finished with value: 0.4814814814814815 and parameters: {'n_estimators': 282, 'max_depth': 6, 'learning_rate': 0.11421438017049272, 'min_child_weight': 4, 'subsample': 0.7943028125454031, 'colsample_bytree': 0.8875045320344611, 'lambda': 9.358547462150582e-07, 'alpha': 9.53867602519731, 'booster': 'gbtree'}. Best is trial 13 with value: 0.5296296296296297.\n",
      "[I 2025-03-24 16:10:03,266] Trial 16 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 198, 'max_depth': 9, 'learning_rate': 0.040497977600932156, 'min_child_weight': 4, 'subsample': 0.8325860224701918, 'colsample_bytree': 0.82364962910557, 'lambda': 3.7054411467957804e-06, 'alpha': 0.40002638158965625, 'booster': 'gbtree'}. Best is trial 16 with value: 0.5333333333333333.\n",
      "[I 2025-03-24 16:10:04,221] Trial 17 finished with value: 0.5296296296296297 and parameters: {'n_estimators': 299, 'max_depth': 9, 'learning_rate': 0.04089301285649145, 'min_child_weight': 5, 'subsample': 0.8556946455640305, 'colsample_bytree': 0.7833786312741762, 'lambda': 1.739584112200154e-05, 'alpha': 0.31276400511920693, 'booster': 'gbtree'}. Best is trial 16 with value: 0.5333333333333333.\n",
      "[I 2025-03-24 16:10:04,855] Trial 18 finished with value: 0.4666666666666666 and parameters: {'n_estimators': 197, 'max_depth': 9, 'learning_rate': 0.031838589470477505, 'min_child_weight': 3, 'subsample': 0.8219116874611772, 'colsample_bytree': 0.6509649275117388, 'lambda': 0.001940262233922398, 'alpha': 0.7204756885669291, 'booster': 'gbtree'}. Best is trial 16 with value: 0.5333333333333333.\n",
      "[I 2025-03-24 16:10:05,558] Trial 19 finished with value: 0.5222222222222223 and parameters: {'n_estimators': 228, 'max_depth': 8, 'learning_rate': 0.061654656137494455, 'min_child_weight': 7, 'subsample': 0.7126636434081005, 'colsample_bytree': 0.8396953578444715, 'lambda': 1.7281102371904842e-07, 'alpha': 0.001660067130149497, 'booster': 'gbtree'}. Best is trial 16 with value: 0.5333333333333333.\n",
      "[I 2025-03-24 16:10:06,714] Trial 20 finished with value: 0.5296296296296297 and parameters: {'n_estimators': 242, 'max_depth': 8, 'learning_rate': 0.017078543020778787, 'min_child_weight': 1, 'subsample': 0.9232421254787151, 'colsample_bytree': 0.9406913083110585, 'lambda': 1.5861235783316702e-05, 'alpha': 0.5755106822844619, 'booster': 'gbtree'}. Best is trial 16 with value: 0.5333333333333333.\n",
      "[I 2025-03-24 16:10:07,707] Trial 21 finished with value: 0.5259259259259259 and parameters: {'n_estimators': 298, 'max_depth': 9, 'learning_rate': 0.04067076387899483, 'min_child_weight': 5, 'subsample': 0.8455386604160793, 'colsample_bytree': 0.7701317513791162, 'lambda': 2.383287502690159e-05, 'alpha': 0.12504574994328624, 'booster': 'gbtree'}. Best is trial 16 with value: 0.5333333333333333.\n",
      "[I 2025-03-24 16:10:08,826] Trial 22 finished with value: 0.537037037037037 and parameters: {'n_estimators': 299, 'max_depth': 10, 'learning_rate': 0.03946174871215509, 'min_child_weight': 5, 'subsample': 0.8992495783237072, 'colsample_bytree': 0.7879434563155175, 'lambda': 0.0005141790009235495, 'alpha': 0.6403997245259925, 'booster': 'gbtree'}. Best is trial 22 with value: 0.537037037037037.\n",
      "[I 2025-03-24 16:10:09,782] Trial 23 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 198, 'max_depth': 10, 'learning_rate': 0.021538119738564683, 'min_child_weight': 3, 'subsample': 0.9976375027908702, 'colsample_bytree': 0.7119724488516133, 'lambda': 0.0008733315719407356, 'alpha': 1.1730570104553013, 'booster': 'gbtree'}. Best is trial 22 with value: 0.537037037037037.\n",
      "[I 2025-03-24 16:10:10,749] Trial 24 finished with value: 0.5185185185185185 and parameters: {'n_estimators': 196, 'max_depth': 10, 'learning_rate': 0.018648532104632572, 'min_child_weight': 3, 'subsample': 0.9955485075173222, 'colsample_bytree': 0.7125241230972144, 'lambda': 0.0007909097341872532, 'alpha': 0.07184393791416147, 'booster': 'gbtree'}. Best is trial 22 with value: 0.537037037037037.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost Accuracy: 0.5147\n",
      "ðŸ•’ Training Time: 0.23 seconds\n",
      "Best Parameters: {'n_estimators': 299, 'max_depth': 10, 'learning_rate': 0.03946174871215509, 'min_child_weight': 5, 'subsample': 0.8992495783237072, 'colsample_bytree': 0.7879434563155175, 'lambda': 0.0005141790009235495, 'alpha': 0.6403997245259925, 'booster': 'gbtree'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75        15\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       0.15      0.15      0.15        13\n",
      "           3       0.62      0.38      0.48        13\n",
      "           4       0.14      0.15      0.15        13\n",
      "\n",
      "    accuracy                           0.51        68\n",
      "   macro avg       0.50      0.50      0.49        68\n",
      "weighted avg       0.51      0.51      0.51        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_optimized.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(eval_metric=\"logloss\", **params)\n",
    "    \n",
    "    # Use Stratified K-Fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in kf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        scores.append(accuracy_score(y_val_fold, y_pred_fold))\n",
    "    \n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# Run Optuna hyperparameter tuning\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=25)  # Reduced trials for faster tuning\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "model = XGBClassifier(eval_metric=\"logloss\", **best_params)\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… XGBoost Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ•’ Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"xgboost_optimized.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomTreeClassifer with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 15:38:47,889] A new study created in memory with name: no-name-e1861716-9b93-49b0-ba64-152cb9daf988\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:38:48,395] Trial 0 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 111, 'max_depth': 16, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 0.6215095707847368}. Best is trial 0 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:38:49,594] Trial 1 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 302, 'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 0.6745821970571525}. Best is trial 0 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:38:50,893] Trial 2 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 324, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 0.7626785631466297}. Best is trial 0 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:38:52,357] Trial 3 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 356, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.7109894214226375}. Best is trial 0 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:38:54,195] Trial 4 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 368, 'max_depth': 18, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 0.6565342053727112}. Best is trial 0 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:38:55,291] Trial 5 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 266, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 0.8381454454298827}. Best is trial 0 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:38:56,255] Trial 6 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 245, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 0.9828998706325667}. Best is trial 0 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:38:58,179] Trial 7 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 455, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 0.5071335392855749}. Best is trial 0 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:38:59,734] Trial 8 finished with value: 0.95 and parameters: {'n_estimators': 367, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 0.5147487321664421}. Best is trial 8 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1512873416.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
      "[I 2025-03-24 15:39:00,312] Trial 9 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 140, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 0.6349939196037084}. Best is trial 8 with value: 0.95.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 1.0000\n",
      "Best Parameters: {'n_estimators': 367, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 0.5147487321664421}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_uniform('max_features', 0.5, 1.0),\n",
    "    }\n",
    "    \n",
    "    model = ExtraTreesClassifier(**params)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    return cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy').mean()\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Train final model\n",
    "best_params = study.best_params\n",
    "model = ExtraTreesClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Final Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 15:34:49,481] A new study created in memory with name: no-name-fdb472bf-3bd6-4fd0-9b3f-312478cc7208\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:34:51,131] Trial 0 finished with value: 0.9166666666666667 and parameters: {'n_estimators': 394, 'max_depth': 16, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': 0.4308665904639004, 'bootstrap': False}. Best is trial 0 with value: 0.9166666666666667.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:34:52,422] Trial 1 finished with value: 0.925 and parameters: {'n_estimators': 235, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 0.33083476586304084, 'bootstrap': True}. Best is trial 1 with value: 0.925.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:34:54,388] Trial 2 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 330, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 10, 'max_features': 0.5226723042141496, 'bootstrap': True}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:34:55,831] Trial 3 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 315, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 0.8639932950124242, 'bootstrap': False}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:34:58,166] Trial 4 finished with value: 0.925 and parameters: {'n_estimators': 413, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 9, 'max_features': 0.18609336718803565, 'bootstrap': True}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:34:59,750] Trial 5 finished with value: 0.9166666666666666 and parameters: {'n_estimators': 348, 'max_depth': 18, 'min_samples_split': 6, 'min_samples_leaf': 10, 'max_features': 0.3739956402716964, 'bootstrap': False}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:01,470] Trial 6 finished with value: 0.925 and parameters: {'n_estimators': 391, 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 0.20728691249869663, 'bootstrap': False}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:03,520] Trial 7 finished with value: 0.925 and parameters: {'n_estimators': 363, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 9, 'max_features': 0.2817017595176417, 'bootstrap': True}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:03,913] Trial 8 finished with value: 0.9166666666666666 and parameters: {'n_estimators': 94, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 8, 'max_features': 0.5260521320810213, 'bootstrap': False}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:04,641] Trial 9 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 124, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 0.3172428528461804, 'bootstrap': True}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:07,588] Trial 10 finished with value: 0.9333333333333332 and parameters: {'n_estimators': 499, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 0.7472814059753483, 'bootstrap': True}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:08,682] Trial 11 finished with value: 0.9333333333333332 and parameters: {'n_estimators': 255, 'max_depth': 4, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 0.9744942857909122, 'bootstrap': False}. Best is trial 2 with value: 0.9416666666666668.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:09,559] Trial 12 finished with value: 0.95 and parameters: {'n_estimators': 183, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.7390463076154373, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:10,656] Trial 13 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 184, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 0.6759247903294562, 'bootstrap': True}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:11,525] Trial 14 finished with value: 0.925 and parameters: {'n_estimators': 182, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 0.6052886417853868, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:11,921] Trial 15 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 59, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 0.7915442660886378, 'bootstrap': True}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:12,729] Trial 16 finished with value: 0.9166666666666667 and parameters: {'n_estimators': 195, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 0.5192528813889807, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:15,429] Trial 17 finished with value: 0.9333333333333332 and parameters: {'n_estimators': 469, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 0.6375869421219647, 'bootstrap': True}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:16,863] Trial 18 finished with value: 0.95 and parameters: {'n_estimators': 305, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.9268959367047733, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:18,150] Trial 19 finished with value: 0.95 and parameters: {'n_estimators': 298, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.989914912848605, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:18,780] Trial 20 finished with value: 0.95 and parameters: {'n_estimators': 138, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.8746715854601264, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:20,123] Trial 21 finished with value: 0.95 and parameters: {'n_estimators': 277, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.9945027009513564, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:21,395] Trial 22 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 287, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.8933049969639271, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:22,504] Trial 23 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 222, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.7766271466825945, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:23,684] Trial 24 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 284, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.9296888944856343, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:24,364] Trial 25 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 148, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 0.8208686476569047, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:25,411] Trial 26 finished with value: 0.95 and parameters: {'n_estimators': 245, 'max_depth': 16, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.7185144332501187, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:26,736] Trial 27 finished with value: 0.9333333333333332 and parameters: {'n_estimators': 303, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 0.9371946552502864, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:27,737] Trial 28 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 211, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.8289743667567016, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:29,619] Trial 29 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 415, 'max_depth': 16, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.9985411287256549, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:31,229] Trial 30 finished with value: 0.925 and parameters: {'n_estimators': 361, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 6, 'max_features': 0.7095698390555596, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:31,832] Trial 31 finished with value: 0.95 and parameters: {'n_estimators': 138, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.8672688574485061, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:32,237] Trial 32 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 89, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.9159117926920058, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:32,988] Trial 33 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 163, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 0.1034337765361592, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:33,523] Trial 34 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 114, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.8493761524288231, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:34,958] Trial 35 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 328, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.930267505967115, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:36,122] Trial 36 finished with value: 0.9416666666666667 and parameters: {'n_estimators': 261, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 0.7910282937504672, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:37,049] Trial 37 finished with value: 0.925 and parameters: {'n_estimators': 215, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 0.43541304943213244, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:38,372] Trial 38 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 310, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 0.885233178929099, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:38,745] Trial 39 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 60, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 0.9603327514903728, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:40,620] Trial 40 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 398, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.8398268981196294, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:41,830] Trial 41 finished with value: 0.95 and parameters: {'n_estimators': 276, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.9731965566391441, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:43,298] Trial 42 finished with value: 0.95 and parameters: {'n_estimators': 335, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.9976220832440966, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:44,432] Trial 43 finished with value: 0.9416666666666668 and parameters: {'n_estimators': 245, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.9006268920572988, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:45,732] Trial 44 finished with value: 0.95 and parameters: {'n_estimators': 297, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.9650275160226324, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:46,960] Trial 45 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 263, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.7572948338829855, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:48,997] Trial 46 finished with value: 0.9333333333333332 and parameters: {'n_estimators': 357, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 0.8735270262565843, 'bootstrap': True}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:49,806] Trial 47 finished with value: 0.95 and parameters: {'n_estimators': 171, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.938202576203618, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:51,857] Trial 48 finished with value: 0.9333333333333332 and parameters: {'n_estimators': 324, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 0.5776066815447803, 'bootstrap': True}. Best is trial 12 with value: 0.95.\n",
      "/tmp/ipykernel_15479/1936857472.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
      "[I 2025-03-24 15:35:52,811] Trial 49 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 229, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.6632913337258515, 'bootstrap': False}. Best is trial 12 with value: 0.95.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 1.0000\n",
      "Best Parameters: {'n_estimators': 183, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.7390463076154373, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=42, **params)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    return cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy').mean()\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Train final model\n",
    "best_params = study.best_params\n",
    "model = RandomForestClassifier(random_state=42, **best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Final Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
