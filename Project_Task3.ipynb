{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost Accuracy: 0.5294\n",
      "üïí Training Time: 52.80 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.64      0.61        11\n",
      "           1       0.92      0.92      0.92        12\n",
      "           2       0.54      0.44      0.48        16\n",
      "           3       0.30      0.55      0.39        11\n",
      "           4       0.45      0.28      0.34        18\n",
      "\n",
      "    accuracy                           0.53        68\n",
      "   macro avg       0.56      0.56      0.55        68\n",
      "weighted avg       0.55      0.53      0.53        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_model.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\",\"V\"])\n",
    "y = df[\"M\"]-1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = XGBClassifier(\n",
    "    eval_metric=\"logloss\",  \n",
    "    booster=\"dart\",  # Dropout-based boosting  \n",
    "    n_estimators=1000,  \n",
    "    max_depth=10,  \n",
    "    learning_rate=0.7,  \n",
    "    subsample=0.8,  \n",
    "    colsample_bytree=0.8,  \n",
    "    reg_lambda=18,\n",
    "    reg_alpha=0.6,  # L2 regularization term (adjust this value as needed)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ XGBoost Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üïí Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"xgboost_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.238816\n",
      "0:\tlearn: 0.5592593\ttotal: 53.4ms\tremaining: 13.3s\n",
      "50:\tlearn: 0.8592593\ttotal: 111ms\tremaining: 432ms\n",
      "100:\tlearn: 0.9703704\ttotal: 157ms\tremaining: 231ms\n",
      "150:\tlearn: 0.9925926\ttotal: 207ms\tremaining: 135ms\n",
      "200:\tlearn: 1.0000000\ttotal: 254ms\tremaining: 62ms\n",
      "249:\tlearn: 1.0000000\ttotal: 298ms\tremaining: 0us\n",
      "‚úÖ CatBoost Accuracy: 0.5882\n",
      "üïí Training Time: 0.38 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.91      0.80        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "           2       0.53      0.50      0.52        16\n",
      "           3       0.40      0.55      0.46        11\n",
      "           4       0.45      0.28      0.34        18\n",
      "\n",
      "    accuracy                           0.59        68\n",
      "   macro avg       0.59      0.63      0.60        68\n",
      "weighted avg       0.58      0.59      0.57        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['catboost_model.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = CatBoostClassifier(iterations=250, eval_metric=\"Accuracy\", verbose=50)\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ CatBoost Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üïí Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"catboost_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logistic Regression Accuracy: 0.4412\n",
      "üïí Training Time: 0.01 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.87      0.65        15\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       0.25      0.15      0.19        13\n",
      "           3       0.00      0.00      0.00        13\n",
      "           4       0.08      0.08      0.08        13\n",
      "\n",
      "    accuracy                           0.44        68\n",
      "   macro avg       0.34      0.42      0.37        68\n",
      "weighted avg       0.36      0.44      0.39        68\n",
      "\n",
      "üéâ Model saved as 'logistic_regression_model.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/FDS_MiniProject/FDS/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V_log\"])\n",
    "y = df[\"M\"] - 1  # Adjust target variable if necessary\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, solver=\"lbfgs\", multi_class=\"auto\")\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üïí Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"logistic_regression_model.pkl\")\n",
    "print(\"üéâ Model saved as 'logistic_regression_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RandomForest Accuracy: 0.5294\n",
      "üïí Training Time: 0.29 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.91      0.77        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "           2       0.42      0.31      0.36        16\n",
      "           3       0.38      0.55      0.44        11\n",
      "           4       0.33      0.22      0.27        18\n",
      "\n",
      "    accuracy                           0.53        68\n",
      "   macro avg       0.53      0.58      0.54        68\n",
      "weighted avg       0.50      0.53      0.51        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['randomforest_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1  # Adjusting target as M - 1\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Objective function to optimize hyperparameters\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters using optuna\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    \n",
    "    # Removing 'auto' and allowing only valid options for max_features\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    \n",
    "    bootstrap = trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "\n",
    "    # Initialize the RandomForest model with suggested hyperparameters\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Create a study to maximize accuracy\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=150)  # You can adjust n_trials for more trials\n",
    "\n",
    "# Output the best hyperparameters and best accuracy\n",
    "print(f\"Best Hyperparameters: {study.best_params}\")\n",
    "print(f\"Best Accuracy: {study.best_value:.4f}\")\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "best_params = study.best_params\n",
    "final_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "y_pred = final_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ Final RandomForest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# Save the final model\n",
    "joblib.dump(final_model, \"best_randomforest_model_optuna.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ExtraTrees Accuracy: 0.4706\n",
      "üïí Training Time: 0.24 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.91      0.74        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "           2       0.25      0.19      0.21        16\n",
      "           3       0.36      0.45      0.40        11\n",
      "           4       0.23      0.17      0.19        18\n",
      "\n",
      "    accuracy                           0.47        68\n",
      "   macro avg       0.46      0.53      0.49        68\n",
      "weighted avg       0.43      0.47      0.44        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['extratrees_model.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\",\"V\"])\n",
    "y = df[\"M\"]-1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = ExtraTreesClassifier(n_estimators=250, random_state=42)\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ ExtraTrees Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üïí Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"extratrees_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SVM Accuracy: 0.3676\n",
      "üïí Training Time: 0.02 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.82      0.37        11\n",
      "           1       1.00      0.92      0.96        12\n",
      "           2       0.20      0.12      0.15        16\n",
      "           3       0.50      0.09      0.15        11\n",
      "           4       0.29      0.11      0.16        18\n",
      "\n",
      "    accuracy                           0.37        68\n",
      "   macro avg       0.44      0.41      0.36        68\n",
      "weighted avg       0.42      0.37      0.33        68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\",\"V\"])\n",
    "y = df[\"M\"]-1\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = SVC(kernel=\"rbf\", probability=True)\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ SVM Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üïí Training Time: {end_time - start_time:.2f} seconds\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"svm_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 3000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
    "    }\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", GradientBoostingClassifier(**params, random_state=42))\n",
    "    ])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best model\n",
    "print(\"‚úÖ Best Accuracy:\", study.best_value)\n",
    "print(\"üìä Best Hyperparameters:\", study.best_params)\n",
    "\n",
    "# Train best model\n",
    "best_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", GradientBoostingClassifier(**study.best_params, random_state=42))\n",
    "])\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Final evaluation\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"\\nüéØ Final Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_model, \"gradientboosting_model_optuna.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import optuna\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1  # Target adjustment\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Optional train-test split for final evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\", [(64,), (128,), (64, 64), (128, 64), (128, 128)])\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"adam\", \"sgd\"])\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-2, log=True)\n",
    "    learning_rate_init = trial.suggest_float(\"learning_rate_init\", 1e-4, 1e-1, log=True)\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 200, 800)\n",
    "\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Use cross-validation\n",
    "    accuracy = cross_val_score(clf, X_scaled, y, cv=5, scoring=\"accuracy\").mean()\n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Output best result\n",
    "print(f\"üîß Best Hyperparameters:\\n{study.best_params}\")\n",
    "print(f\"üèÜ Best CV Accuracy: {study.best_value:.4f}\")\n",
    "\n",
    "# Train final model with best params\n",
    "best_params = study.best_params\n",
    "final_model = MLPClassifier(**best_params, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate final model\n",
    "y_pred = final_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ Final MLP Accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nüìä Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(final_model, \"best_mlp_model_optuna.pkl\")\n",
    "joblib.dump(scaler, \"mlp_scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPClassifier Model(with higher accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1  # Adjusted target labels\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Use best hyperparameters from Optuna study\n",
    "best_params = {\n",
    "    'hidden_layer_sizes': (64, 64),\n",
    "    'activation': 'tanh',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0009747463996145735,\n",
    "    'learning_rate_init': 0.0018593430004181788,\n",
    "    'max_iter': 676\n",
    "}\n",
    "\n",
    "# Initialize and train the MLP model\n",
    "mlp_model = MLPClassifier(**best_params, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test data\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ Final MLP Accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nüìä Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# Save the final model and scaler\n",
    "joblib.dump(mlp_model, \"final_mlp_model.pkl\")\n",
    "joblib.dump(scaler, \"mlp_scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Final MLP model with best Optuna hyperparameters\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 128, 64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=0.00022155272099481024,\n",
    "    learning_rate_init=0.038373306648018864,\n",
    "    max_iter=645,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"‚úÖ Final MLP Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"\\nüìä Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(mlp, \"mlp_final_optuna_relu.pkl\")\n",
    "joblib.dump(scaler, \"scaler_final_optuna_relu.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP_Classifier Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Optuna objective function with pruning\n",
    "def objective(trial):\n",
    "    hidden_layer_sizes = trial.suggest_categorical(\n",
    "        \"hidden_layer_sizes\", [(128, 128), (256, 128), (256, 128), (256, 128, 64), (256, 128, 64)]\n",
    "    )\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-2, log=True)\n",
    "    learning_rate_init = trial.suggest_float(\"learning_rate_init\", 2e-4, 1e-1, log=True)\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 300, 800)\n",
    "\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Report intermediate accuracy and prune if necessary\n",
    "    trial.report(accuracy, step=1)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna study with pruning\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=100, timeout=600)  # Add timeout or other limits if needed\n",
    "\n",
    "# Print best params\n",
    "print(\"üîß Best Hyperparameters:\")\n",
    "print(study.best_params)\n",
    "print(f\"üèÜ Best Accuracy: {study.best_value:.4f}\")\n",
    "\n",
    "# Train final model\n",
    "best_params = study.best_params\n",
    "final_model = MLPClassifier(\n",
    "    **best_params,\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix using seaborn for classes 0 to 4\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[f'Pred Class {i}' for i in range(5)], \n",
    "            yticklabels=[f'True Class {i}' for i in range(5)])\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Classes 0 to 4)')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Final MLP Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"\\nüìä Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(final_model, \"mlp_optuna_best_relu.pkl\")\n",
    "joblib.dump(scaler, \"scaler_optuna_best_relu.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed_land_mines.csv\")\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df.drop(columns=[\"M\", \"V\"])\n",
    "y = df[\"M\"] - 1  # Shift labels from 1‚Äì5 to 0‚Äì4\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Define and train the final MLP model\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128),\n",
    "    alpha=1.5533844813198423e-05,\n",
    "    learning_rate_init=0.012213599390700235,\n",
    "    max_iter=587,\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"‚úÖ Final MLP Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    clf, X_test, y_test, display_labels=[0, 1, 2, 3, 4], cmap=\"Blues\"\n",
    ")\n",
    "plt.title(\"Confusion Matrix - MLPClassifier\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(clf, \"mlp_optuna_best_relu.pkl\")\n",
    "joblib.dump(scaler, \"scaler_optuna_best_relu.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
